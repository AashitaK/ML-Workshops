{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import python modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will learn regression techniques using [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview) dataset. The aim is to predict house prices based on a set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities    ...     PoolArea PoolQC Fence MiscFeature MiscVal  \\\n",
       "0         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "1         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "2         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "3         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "4         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "\n",
       "  MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0      2   2008        WD         Normal     208500  \n",
       "1      5   2007        WD         Normal     181500  \n",
       "2      9   2008        WD         Normal     223500  \n",
       "3      2   2006        WD        Abnorml     140000  \n",
       "4     12   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'house-prices-advanced-regression-techniques/'\n",
    "housing = pd.read_csv(path + 'train.csv')\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 81)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are ***81 columns in total***. Before looking deeply into the columns, we might want to first discard the columns that have a lot of missing values. We can revisit later to include and process all the columns, but at first we would want to work with fewer columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                  0\n",
       "MSSubClass          0\n",
       "MSZoning            0\n",
       "LotFrontage       259\n",
       "LotArea             0\n",
       "Street              0\n",
       "Alley            1369\n",
       "LotShape            0\n",
       "LandContour         0\n",
       "Utilities           0\n",
       "LotConfig           0\n",
       "LandSlope           0\n",
       "Neighborhood        0\n",
       "Condition1          0\n",
       "Condition2          0\n",
       "BldgType            0\n",
       "HouseStyle          0\n",
       "OverallQual         0\n",
       "OverallCond         0\n",
       "YearBuilt           0\n",
       "YearRemodAdd        0\n",
       "RoofStyle           0\n",
       "RoofMatl            0\n",
       "Exterior1st         0\n",
       "Exterior2nd         0\n",
       "MasVnrType          8\n",
       "MasVnrArea          8\n",
       "ExterQual           0\n",
       "ExterCond           0\n",
       "Foundation          0\n",
       "                 ... \n",
       "BedroomAbvGr        0\n",
       "KitchenAbvGr        0\n",
       "KitchenQual         0\n",
       "TotRmsAbvGrd        0\n",
       "Functional          0\n",
       "Fireplaces          0\n",
       "FireplaceQu       690\n",
       "GarageType         81\n",
       "GarageYrBlt        81\n",
       "GarageFinish       81\n",
       "GarageCars          0\n",
       "GarageArea          0\n",
       "GarageQual         81\n",
       "GarageCond         81\n",
       "PavedDrive          0\n",
       "WoodDeckSF          0\n",
       "OpenPorchSF         0\n",
       "EnclosedPorch       0\n",
       "3SsnPorch           0\n",
       "ScreenPorch         0\n",
       "PoolArea            0\n",
       "PoolQC           1453\n",
       "Fence            1179\n",
       "MiscFeature      1406\n",
       "MiscVal             0\n",
       "MoSold              0\n",
       "YrSold              0\n",
       "SaleType            0\n",
       "SaleCondition       0\n",
       "SalePrice           0\n",
       "Length: 81, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We discard all the columns that have more than 100 missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = housing.loc[:, housing.isnull().sum() < 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the remaining columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'MSSubClass', 'MSZoning', 'LotArea', 'Street', 'LotShape',\n",
       "       'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood',\n",
       "       'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'OverallQual',\n",
       "       'OverallCond', 'YearBuilt', 'YearRemodAdd', 'RoofStyle', 'RoofMatl',\n",
       "       'Exterior1st', 'Exterior2nd', 'MasVnrType', 'MasVnrArea', 'ExterQual',\n",
       "       'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure',\n",
       "       'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF',\n",
       "       'TotalBsmtSF', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical',\n",
       "       '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath',\n",
       "       'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr',\n",
       "       'KitchenQual', 'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'GarageType',\n",
       "       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',\n",
       "       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n",
       "       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal',\n",
       "       'MoSold', 'YrSold', 'SaleType', 'SaleCondition', 'SalePrice'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how does the columns correlate with the *SalePrice* using the Pearson correlation coefficients that we learned in the first session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id              -0.021917\n",
       "MSSubClass      -0.084284\n",
       "LotArea          0.263843\n",
       "OverallQual      0.790982\n",
       "OverallCond     -0.077856\n",
       "YearBuilt        0.522897\n",
       "YearRemodAdd     0.507101\n",
       "MasVnrArea       0.477493\n",
       "BsmtFinSF1       0.386420\n",
       "BsmtFinSF2      -0.011378\n",
       "BsmtUnfSF        0.214479\n",
       "TotalBsmtSF      0.613581\n",
       "1stFlrSF         0.605852\n",
       "2ndFlrSF         0.319334\n",
       "LowQualFinSF    -0.025606\n",
       "GrLivArea        0.708624\n",
       "BsmtFullBath     0.227122\n",
       "BsmtHalfBath    -0.016844\n",
       "FullBath         0.560664\n",
       "HalfBath         0.284108\n",
       "BedroomAbvGr     0.168213\n",
       "KitchenAbvGr    -0.135907\n",
       "TotRmsAbvGrd     0.533723\n",
       "Fireplaces       0.466929\n",
       "GarageYrBlt      0.486362\n",
       "GarageCars       0.640409\n",
       "GarageArea       0.623431\n",
       "WoodDeckSF       0.324413\n",
       "OpenPorchSF      0.315856\n",
       "EnclosedPorch   -0.128578\n",
       "3SsnPorch        0.044584\n",
       "ScreenPorch      0.111447\n",
       "PoolArea         0.092404\n",
       "MiscVal         -0.021190\n",
       "MoSold           0.046432\n",
       "YrSold          -0.028923\n",
       "SalePrice        1.000000\n",
       "Name: SalePrice, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations = housing.corr()['SalePrice']\n",
    "correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us select only those columns that have a correlation co-efficient of 0.5 or higher in magnitude with the *SalePrice* column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OverallQual     0.790982\n",
       "YearBuilt       0.522897\n",
       "YearRemodAdd    0.507101\n",
       "TotalBsmtSF     0.613581\n",
       "1stFlrSF        0.605852\n",
       "GrLivArea       0.708624\n",
       "FullBath        0.560664\n",
       "TotRmsAbvGrd    0.533723\n",
       "GarageCars      0.640409\n",
       "GarageArea      0.623431\n",
       "SalePrice       1.000000\n",
       "Name: SalePrice, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations[(correlations > 0.5) | (correlations < -0.5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pick a few columns for now to train a model. We can anytime revisit and try a different set of columns for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = housing['SalePrice']\n",
    "\n",
    "predictor_cols = ['OverallQual', 'YearBuilt', \n",
    "                  'YearRemodAdd', 'TotalBsmtSF', \n",
    "                  '1stFlrSF', 'GrLivArea', \n",
    "                  'FullBath', 'TotRmsAbvGrd', \n",
    "                  'GarageCars', 'GarageArea',\n",
    "                 'Fireplaces', 'LotArea']\n",
    "\n",
    "X = housing[predictor_cols]\n",
    "# X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Features' description](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data):\n",
    "* SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n",
    "* OverallQual: Overall material and finish quality\n",
    "* YearBuilt: Original construction date\n",
    "* YearRemodAdd: Remodel date\n",
    "* TotalBsmtSF: Total square feet of basement area\n",
    "* 1stFlrSF: First Floor square feet\n",
    "* GrLivArea: Above grade (ground) living area square feet\n",
    "* FullBath: Full bathrooms above grade\n",
    "* TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n",
    "* GarageCars: Size of garage in car capacity\n",
    "* GarageArea: Size of garage in square feet\n",
    "* Fireplaces: Number of fireplaces\n",
    "* LotArea: Lot size in square feet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we split the data into training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y,\n",
    "                                        random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train a linear regression model using the training set and calculate the $R^2$ score for both training and validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared score (training): 0.812\n",
      "R-squared score (validation): 0.677\n"
     ]
    }
   ],
   "source": [
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('R-squared score (validation): {:.3f}'\n",
    "     .format(linreg.score(X_valid, y_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we try polynomial regression model, which is similar to linear regression but it uses the features with higher degrees. Polynomial regression generate more complex than linear regression and can be very useful in regression models.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/8/8a/Gaussian_kernel_regression.png\" width=\"300\" height=\"350\" />\n",
    "<p style=\"text-align: center;\"> Regression curve </p> \n",
    "\n",
    "For a single feature $x$ with only quadratic (degree 2) terms, the polynomial regression equation would be $ y_{pred} = b + w_1 * x +w_2 * x^2 $. So, it can also be thought of as linear regression with two variables $x$ and $x^2$. Same is true for all polynomial regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We transform the original input features using [`sklearn.preprocessing.PolynomialFeatures`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) to add polynomial features up to degree 2 (quadratic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "X_train_poly, X_valid_poly, y_train_poly, y_valid_poly = train_test_split(X_poly, y,\n",
    "                                                   random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the built-in [`LinearRegression()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) with the polynomial features to build a polynomial regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared score (training): 0.870\n",
      "R-squared score (validation): 0.810\n"
     ]
    }
   ],
   "source": [
    "polyreg = LinearRegression().fit(X_train_poly, y_train_poly)\n",
    "\n",
    "polyreg_train_score = polyreg.score(X_train_poly, y_train_poly)\n",
    "polyreg_valid_score = polyreg.score(X_valid_poly, y_valid_poly)\n",
    "\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(polyreg_train_score))\n",
    "print('R-squared score (validation): {:.3f}'\n",
    "     .format(polyreg_valid_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is great to see that the performance has increased significantly by using polynomial features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On account of increased complexity, polynomial regression models are prone to overfitting. So,  they are often coupled with Ridge regression or Lasso regression - extensions of linear regression algorithm that uses regularization, a method to address overfitting.\n",
    "\n",
    "#### Regularization:\n",
    "Regularization adds a penalty term called regularization parameter (alpha) to penalize the weights keeping them smaller and making the model simpler.\n",
    "\n",
    "To the linear regression formulation, we add the model weights multiplied by the regularization parameter (alpha) to the cost function so that when the learning process minimizes the cost function while updating the weights, it automatically keeps the weights in check. There are two common ways to add the weights term (using $L1$ and $L2$-norms) and hence the two algorithms.\n",
    "\n",
    "Cost function for Linear regression:\n",
    "$$ J = \\frac{1}{2 n} \\sum_{i=1}^n (y^{(i)} - y_{pred}^{(i)})^2 $$\n",
    "\n",
    "Cost function for Ridge regression ($L2$-norm):\n",
    "$$ J = \\frac{1}{2 n} \\sum_{i=1}^n (y^{(i)} - y_{pred}^{(i)})^2 + \\alpha \\sum_{j=1}^m w_j^2$$\n",
    "\n",
    "Cost function for Lasso regression ($L1$-norm):\n",
    "$$ J = \\frac{1}{2 n} \\sum_{i=1}^n (y^{(i)} - y_{pred}^{(i)})^2 + \\alpha \\sum_{j=1}^m |w_j|$$\n",
    "\n",
    "Regularization along with drop out are most commonly used and crucial techniques to address overfitting in deep neural networks and it works the same way by adding weight term to the cost function using either $L1$ or $L2$-normalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared score (training): 0.890\n",
      "R-squared score (validation): 0.797\n"
     ]
    }
   ],
   "source": [
    "polyreg_lasso = Lasso(alpha=100).fit(X_train_poly, y_train_poly)\n",
    "\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(polyreg_lasso.score(X_train_poly, y_train_poly)))\n",
    "print('R-squared score (validation): {:.3f}'\n",
    "     .format(polyreg_lasso.score(X_valid_poly, y_valid_poly)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared score (training): 0.893\n",
      "R-squared score (validation): 0.814\n"
     ]
    }
   ],
   "source": [
    "polyreg_ridge = Ridge(alpha=100).fit(X_train_poly, y_train_poly)\n",
    "\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(polyreg_ridge.score(X_train_poly, y_train_poly)))\n",
    "print('R-squared score (validation): {:.3f}'\n",
    "     .format(polyreg_ridge.score(X_valid_poly, y_valid_poly)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared score (training): 0.886\n",
      "R-squared score (validation): 0.832\n"
     ]
    }
   ],
   "source": [
    "polyreg_elasticnet = ElasticNet(alpha=200).fit(X_train_poly, y_train_poly)\n",
    "\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(polyreg_elasticnet.score(X_train_poly, y_train_poly)))\n",
    "print('R-squared score (validation): {:.3f}'\n",
    "     .format(polyreg_elasticnet.score(X_valid_poly, y_valid_poly)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mean Squared Logarithmic Error cannot be used when targets contain negative values.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-a0339c07a7ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0malphas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m75\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m750\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mscores_lasso\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlasso_validation_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mscores_lasso_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscores_lasso\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mscores_lasso_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscores_lasso\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-a0339c07a7ac>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0malphas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m75\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m750\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mscores_lasso\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlasso_validation_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mscores_lasso_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscores_lasso\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mscores_lasso_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscores_lasso\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-a0339c07a7ac>\u001b[0m in \u001b[0;36mlasso_validation_curve\u001b[0;34m(alpha)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLasso\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_poly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_poly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtrain_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtrain_rmsle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_rmsle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_rmsle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_rmsle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_rmsle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-a0339c07a7ac>\u001b[0m in \u001b[0;36mget_rmsle\u001b[0;34m(reg)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_rmsle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_log_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_poly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0my_pred_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid_poly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mvalid_rmsle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_log_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid_poly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_rmsle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_rmsle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36mmean_squared_log_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         raise ValueError(\"Mean Squared Logarithmic Error cannot be used when \"\n\u001b[0m\u001b[1;32m    315\u001b[0m                          \"targets contain negative values.\")\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Mean Squared Logarithmic Error cannot be used when targets contain negative values."
     ]
    }
   ],
   "source": [
    "def get_scores(reg):\n",
    "    train_score = reg.score(X_train_poly, y_train_poly)\n",
    "    valid_score = reg.score(X_valid_poly, y_valid_poly)\n",
    "    return train_score, valid_score\n",
    "\n",
    "def get_rmsle(reg):\n",
    "    y_pred_train = reg.predict(X_train_poly)\n",
    "    train_rmsle = np.sqrt(mean_squared_log_error(y_train_poly, y_pred_train))\n",
    "    y_pred_valid = reg.predict(X_valid_poly)\n",
    "    valid_rmsle = np.sqrt(mean_squared_log_error(y_valid_poly, y_pred_valid))\n",
    "    return train_rmsle, valid_rmsle\n",
    "\n",
    "def ridge_validation_curve(alpha):\n",
    "    reg = Ridge(alpha=alpha).fit(X_train_poly, y_train_poly)\n",
    "    train_score, valid_score = get_scores(reg)\n",
    "    train_rmsle, valid_rmsle = get_rmsle(reg)  \n",
    "    return train_score, valid_score, train_rmsle, valid_rmsle\n",
    "\n",
    "def lasso_validation_curve(alpha):\n",
    "    reg = Lasso(alpha=alpha).fit(X_train_poly, y_train_poly)\n",
    "    train_score, valid_score = get_scores(reg)\n",
    "    train_rmsle, valid_rmsle = get_rmsle(reg)  \n",
    "    return train_score, valid_score, train_rmsle, valid_rmsle\n",
    "\n",
    "def elasticnet_validation_curve(alpha):\n",
    "    reg = ElasticNet(alpha=alpha).fit(X_train_poly, y_train_poly)\n",
    "    train_score, valid_score = get_scores(reg)\n",
    "    train_rmsle, valid_rmsle = get_rmsle(reg)  \n",
    "    return train_score, valid_score, train_rmsle, valid_rmsle\n",
    "\n",
    "alphas = [0.1, 1, 5, 25, 50, 75, 100, 200, 300, 400, 500, 750, 1000, 2000]\n",
    "\n",
    "scores_lasso = [lasso_validation_curve(alpha) for alpha in alphas]\n",
    "scores_lasso_train = [s[0] for s in scores_lasso]\n",
    "scores_lasso_valid = [s[1] for s in scores_lasso]\n",
    "rmsle_lasso_train = [s[2] for s in scores_lasso]\n",
    "rmsle_lasso_valid = [s[3] for s in scores_lasso]\n",
    "\n",
    "scores_ridge = [ridge_validation_curve(alpha) for alpha in alphas]\n",
    "scores_ridge_train = [s[0] for s in scores_ridge]\n",
    "scores_ridge_valid = [s[1] for s in scores_ridge]\n",
    "rmsle_ridge_train = [s[2] for s in scores_ridge]\n",
    "rmsle_ridge_valid = [s[3] for s in scores_ridge]\n",
    "\n",
    "scores_elasticnet = [elasticnet_validation_curve(alpha) for alpha in alphas]\n",
    "scores_elasticnet_train = [s[0] for s in scores_elasticnet]\n",
    "scores_elasticnet_valid = [s[1] for s in scores_elasticnet]\n",
    "rmsle_elasticnet_train = [s[2] for s in scores_elasticnet]\n",
    "rmsle_elasticnet_valid = [s[3] for s in scores_elasticnet]\n",
    "\n",
    "scores_poly_train = [polyreg_train_score]*len(alphas)\n",
    "scores_poly_valid = [polyreg_valid_score]*len(alphas)\n",
    "y_pred_train = polyreg.predict(X_train_poly)\n",
    "rmsle_poly_train = [mean_squared_error(y_train_poly, y_pred_train)]*len(alphas)\n",
    "y_pred_valid = polyreg.predict(X_valid_poly)\n",
    "rmsle_poly_valid = [mean_squared_error(y_valid_poly, y_pred_valid)]*len(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Elastic Net (train)': scores_elasticnet_train,\n",
    "              'Elastic Net (validation)': scores_elasticnet_valid,\n",
    "              'Ridge (train)': scores_ridge_train,\n",
    "              'Ridge (validation)': scores_ridge_valid,\n",
    "              'Lasso (train)': scores_lasso_train,\n",
    "              'Lasso (validation)': scores_lasso_valid,\n",
    "              'Polynomial (train)': scores_poly_train,\n",
    "              'Polynomial (validation)': scores_poly_valid }, \n",
    "             index=alphas).rename_axis('alpha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6));\n",
    "plt.ylim([0.74, 0.9])\n",
    "plt.xlabel('Regularization parameter (alpha)')\n",
    "plt.ylabel('R-squared')\n",
    "plt.title('R-squared scores as function of regularization')\n",
    "\n",
    "plt.plot(alphas, scores_ridge_train, label='Poynomial with Ridge (training)')\n",
    "plt.plot(alphas, scores_lasso_train, label='Poynomial with Lasso (training)')\n",
    "plt.plot(alphas, scores_elasticnet_train, label='Poynomial with Elastic Net (training)')\n",
    "plt.plot(alphas, scores_poly_train, label='Polynomial (training)')\n",
    "\n",
    "plt.plot(alphas, scores_elasticnet_valid, label='Poynomial with Elastic Net (validation)')\n",
    "plt.plot(alphas, scores_ridge_valid, label='Poynomial with Ridge (validation)')\n",
    "plt.plot(alphas, scores_lasso_valid, label='Poynomial with Lasso (validation)')\n",
    "plt.plot(alphas, scores_poly_valid, label='Polynomial (validation)')\n",
    "plt.legend(loc=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial regression model regularized using Elastic Net seems to be performing the best followed by Lasso by looking at the $R^2$ on the test set.\n",
    "Considering the $R^2$ values on the validation set, the order of performance from the best to worse changes is:\n",
    "1. Polynomial regression coupled with Elastic Net\n",
    "2. Polynomial regression coupled with Ridge \n",
    "3. Polynomial regression coupled with Lasso\n",
    "4. Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also look at the mean-squared error(MSE), which also happens to be the cost function for linear regression as we learned earlier. We will use [`sklearn.metrics.mean_squared_error`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) function to calculate MSE and plot a graph to compare the MSE values for the above four models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11, 6));\n",
    "plt.ylim([100000000, 1500000000])\n",
    "plt.xlabel('Regularization parameter (alpha)')\n",
    "plt.ylabel('Root Mean Squared Logarithmic Error (RMSLE)')\n",
    "plt.title('Root Mean Squared Logarithmic Error (RMSLE) as function of regularization')\n",
    "\n",
    "plt.plot(alphas, rmsle_poly_valid, label='Polynomial (validation)')\n",
    "plt.plot(alphas, rmsle_lasso_valid, label='Poynomial with Lasso (validation)')\n",
    "plt.plot(alphas, rmsle_ridge_valid, label='Poynomial with Ridge (validation)')\n",
    "plt.plot(alphas, rmsle_elasticnet_valid, label='Poynomial with Elastic Net (validation)')\n",
    "\n",
    "plt.plot(alphas, rmsle_poly_train, label='Poynomial (training)')\n",
    "plt.plot(alphas, rmsle_elasticnet_train, label='Poynomial with Elastic Net (training)')\n",
    "plt.plot(alphas, rmsle_lasso_train, label='Poynomial with Lasso (training)')\n",
    "plt.plot(alphas, rmsle_ridge_train, label='Poynomial with Ridge (training)')\n",
    "\n",
    "plt.legend(loc=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the Root Mean Squared Logarithmic Error (RMSLE) on the validation set, it seems that the order of performance from the best to worse is:\n",
    "1. Polynomial regression coupled with Elastic Net\n",
    "2. Polynomial regression coupled with Ridge \n",
    "3. Polynomial regression coupled with Lasso\n",
    "4. Polynomial regression\n",
    "\n",
    "Elastic Net that has the least difference between the RMSLE for training and validation set also has the lowest RMSLE on the validation set in comparison with others, which suggests it is likely not overfitting to the training set. Ditto for the $R^2$ scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we finally predict the examples in the `test.csv` files and [submit our predict to the competition](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview/frequently-asked-questions). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_test = pd.read_csv(path + 'test.csv')\n",
    "Id = housing_test['Id']\n",
    "X_test = housing_test[predictor_cols]\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in missing values for the columns with something you find appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we choose the Elastic Net regression and use the best value for alpha we found so far, that is 2000 and then train the model using the ***entire data set*** transformed by the polynomial features, that is `X_poly, y`.\n",
    "\n",
    "Note that: We don't anymore need to keep the validation set out of training since we have chosen the regression algorithm and the alpha value already, so we are better off using the entire set for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = ElasticNet(alpha=2000).fit(X_poly, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we transform the test data features to the quadratic polynomial features using the transformer `poly` that we fit earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.fillna(method='ffill')\n",
    "X_test_poly = poly.transform(X_test)\n",
    "predictions = reg.predict(X_test_poly)\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a look at the sample submission. It is important that our submission file is in correct format to be graded without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(path + 'sample_submission.csv')\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dataframe for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'Id': Id,\n",
    "                          'SalePrice': predictions})\n",
    "\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the dataframe as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('my_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to the [competition page](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview/evaluation) and make a submission by uploading the `my_submission.csv` file generated thru this notebook that must be stored in the same folder as this notebook in your laptop.\n",
    "\n",
    "Note: If you are using Kaggle kernels, then\n",
    "1. Commit the notebook\n",
    "2. Leave the edit mode\n",
    "3. Navigate to the output section\n",
    "4. Download the file\n",
    "    CHECK AND EDIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional:\n",
    "\n",
    "Problem 1: Feature engineering\n",
    "1. Try out different sets of columns, even including those that have missing values by filling them first. You might want to explore using polynomial features of higher or lower degrees than 2 and choose the regularization accordingly.\n",
    "\n",
    "Problem 2: Evaluation metric \n",
    "1. Define a function to calculate the [metric used for evaluating the submissions for the competition](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview/evaluation):\n",
    "\n",
    "> Submissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)\n",
    "\n",
    "2. Calculate and plot the scores for the above four model based on this metric  as a function of regularization parameter (alpha) to compare their performances.\n",
    "\n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[R-squared](http://www.fairlynerdy.com/what-is-r-squared/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Must do:\n",
    "- Add explanation\n",
    "- More about R2 and MSE both\n",
    "- Elastic Net\n",
    "- Max. voting\n",
    "- Submit and check\n",
    "\n",
    "Maybe:\n",
    "- Noise and regularization, say something\n",
    "- Lasso makes features go to zero, add blog\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acknowledgements to Coursera course and images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
